# 学习笔记





### 1.SVD  LFM

矩阵分解（特征分解），特征分解的目的是给矩阵降低维度。

优点：简化数据、去除噪声，==提高算法的结果？==

缺点：分解的结果的可解释性低

通过SVD（需要填充）对数据的处理，我们**可以使用小得多的数据集来表示原始数据集**，这样做实际上是去除了噪声和冗余信息，以此达到了优化数据、提高结果的目的

LFM(不需要填充)

### .贝叶斯公式

$$
P(A|B)= \frac{P(B|A)P(A)}{P(B)}（1）
$$

or
$$
P(A|B)= \frac{P(B|A)P(A)}{P(B|A)P(A)+P(B|\sim A)P(\sim A)}（2）
$$
这里的$P(A)$就是说的先验概率，$P(A|B)$就是后验概率

**贝叶斯公式就是在描述，你有多大把握能相信一件证据？**

**eg:**

我们假设响警报的目的就是想说汽车被砸了。把A计作“汽车被砸了”，B计作“警报响了”，带进贝叶斯公式里看。我们想求等式左边发生A|B的概率，这是在说警报响了，汽车也确实被砸了。汽车被砸**引起（trigger）**警报响，即B|A。但是，也有可能是汽车被小孩子皮球踢了一下、被行人碰了一下等其他原因（统统计作∼A），其他原因引起汽车警报响了，即B|∼A。那么，现在突然听见警报响了，这时汽车已经被砸了的概率是多少呢（这即是说，警报响这个*证据*有了，多大把握能相信它确实是在报警说汽车被砸了）？想一想，应当这样来计算。用警报响起、汽车也被砸了这事件的数量，除以响警报事件的数量（这即【式1】）。进一步展开，即警报响起、汽车也被砸了的事件的数量，除以警报响起、汽车被砸了的事件数量加上警报响起、汽车没被砸的事件数量（这即【式2】）。

可能有点绕，请稍稍想一想。

再思考【式2】。想让P(A|B)=1，即警报响了，汽车一定被砸了，该怎么做呢？让$P(B|∼A)P(∼A)=0$即可。很容易想清楚，假若让$P(∼A)P(∼A)=0$，即杜绝了汽车被球踢、被行人碰到等等其他所有情况，那自然，警报响了，只剩下一种可能——汽车被砸了。这即是提高了响警报这个证据的说服力。

**从这个角度总结贝叶斯公式：做判断的时候，要考虑所有的因素。** 老板骂你，不一定是你把什么工作搞砸了，可能只是他今天出门前和太太吵了一架。

再思考【式2】。观察【式2】右边的分子，$P(B|A)$为汽车被砸后响警报的概率。姑且仍为这是1吧。但是，若$P(A)$很小，即汽车被砸的概率本身就很小，则$P(B|A)P(A)$仍然很小，即【式2】右边分子仍然很小，P(A|B) 还是大不起来。 这里，P(A)即是常说的先验概率，如果A的先验概率很小，就算P(B|A)较大，可能A的后验概率P(A|B)还是不会大（假设$P(B|∼A)P(∼A)$不变的情况下）。

### .朴素贝叶斯

注：朴素贝叶斯和贝叶斯估计是不同的概念。

### .极大似然估计/最大似然估计（MLE）

==注：相同的概念，不同的说法==

极大似然估计，通俗理解来说，**就是利用已知的样本结果信息，反推最具有可能（最大概率）导致这些样本结果出现的模型参数值！**

**换句话说，极大似然估计提供了一种给定观察数据来评估模型参数的方法，即：“模型已定，参数未知”**

**极大似然估计中采样需满足一个重要的假设，就是所有的采样都是独立同分布的。**

##### A.似然函数

对于$P(x|\theta)$这个函数而言，输入有两个：x表示的是一个具体的数据，$\theta$表示模型的参数

**1.如果$\theta$是已知确定的，$x$是变量，这个函数叫做概率函数(probability function)，它描述对于不同的样本点$x$，其出现概率是多少 **

**2.如果$x$是已知确定的，$θ$是变量，这个函数叫做似然函数(likelihood function), 它描述对于不同的模型参数，出现$x$这个样本点的概率是多少 **

**eg:**

例如，$f(x,y)=x^y$, 即x的y次方。如果x是已知确定的(例如x=2)，这就是$f(y)=2^y$ 这是指数函数。 如果y是已知确定的(例如y=2)，这就是$f(x)=x^2$，这是二次函数。同一个数学形式，从不同的变量角度观察，可以有不同的名字。

##### A.先验概率

先验概率（prior probability）是指根据以往经验和分析得到的概率，它往往作为"由因求果"问题中的"因"出现的概率。例如，先验概率分布可能代表在将来的选举中投票给特定政治家的选民相对比例的概率分布。未知的数量可以是模型的参数或者是潜在变量。

##### B.后验概率

后验概率是指在得到“结果”的信息后重新修正的概率，是“执果寻因”问题中的"果"。是在相关证据或者背景给定并纳入考虑之后的==条件概率==。  后验概率是关于参数 $\theta$ 在给定的证据信息 $X$ 下的概率： $p(θ|x)$。

eg:

假设我们出门堵车的可能因素有两个（就是假设而已，别当真）：车辆太多和交通事故。

- 依照先前经验得到的堵车的概率就是先验概率 。
- 如果我们已经出了门，然后遇到了堵车，那么我们想算一下堵车时由交通事故引起的概率有多大，那这个就叫做后验概率 （也是条件概率，但是通常习惯这么说） 。也就是P(交通事故|堵车)。这是有果求因,
  $P(\theta |X)$就说我们已经知道X了对$\theta$的估计

##### C.极大似然估计

假设有一个造币厂生产某种硬币，现在我们拿到了一枚这种硬币，想试试这硬币是不是均匀的。即想知道抛这枚硬币，正反面出现的概率（记为θ）各是多少？
这是一个统计问题，回想一下，解决统计问题需要什么？ 数据！
于是我们拿这枚硬币抛了10次，得到的数据（x0）是：反正正正正反正正正反。我们想求的正面概率θ是模型参数，而抛硬币模型我们可以假设是 二项分布那么，出现实验结果x0（即反正正正正反正正正反）的似然函数是多少呢？
$$
f(x_0,\theta)=(1-\theta )*\theta *\theta *\theta *\theta *（1-\theta）*\theta *\theta*\theta *（1-\theta）=\theta ^7{(1-\theta)}^3=f(\theta)
$$
最大似然估计就是最大化这个似然函数，$f(\theta)$的图像：

![img](https://img-blog.csdn.net/20170531003926799?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMTUwODY0MA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

可以看出，在θ=0.7时，似然函数取得最大值。这样，我们已经完成了对θ的最大似然估计。即，抛10次硬币，发现7次硬币正面向上，最大似然估计认为正面向上的概率是0.7。（ummm..这非常直观合理，对吧？）

且慢，一些人可能会说，硬币一般都是均匀的啊！ 就算你做实验发现结果是“反正正正正反正正正反”，我也不信θ=0.7。

这里就包含了贝叶斯学派的思想了——要考虑先验概率。 为此，引入了最大后验概率估计

### 最大后验概率估计(MAP)

最大似然估计是求参数θ, 使似然函数P(x0|θ)最大。最大后验概率估计则是想求θ使$P(x0|θ)P(θ)$最大。求得的θ不单单让似然函数大，θ自己出现的先验概率也得大。 （这有点像正则化里加惩罚项的思想，不过正则化里是利用加法，而MAP里是利用乘法）

MAP就是最大化$P(\theta |x_0)= \frac{P(x_0|\theta )P(\theta )}{P(x_0)}$不过$X_0$是确定的，（即投出的“反正正正正反正正正反”），P(x0)是一个已知值，所以去掉了分母P(x0)（假设“投10次硬币”是一次实验，实验做了1000次，“反正正正正反正正正反”出现了n次，则P(x0)=n/1000。总之，这是一个可以由数据集得到的值），最大化P(θ|x0)的意义也很明确，x0已经出现了，要求θ取什么值使P(θ|x0)最大

## 指示函数（indicator function）

指示函数或示性函数（indicator function）数学中，指示函数是定义在某集合X上的函数，表示其中有哪些元素属于某一子集A:
$$
X \longrightarrow \{0,1\} \\\
L_(x)= \begin{cases}
1,if\quad x\in A \\
0, otherwise
\end{cases}
\tag{1}

$$
即若x为真，则取值为1，否则取值为0.

## 贝叶斯判定准则（Bayes decision rule）

为最小化总体风险（损失），只需要在每个样本上选择能使条件风险$R(c|x)$最小的类别标记，其中c为标签，x为数据

## （期望极大算法）EM

#### A.隐变量

举个例子吧

一个人拿着n个袋子，里面有m种颜色不同的球。现在这个人随机地抓球，规则如下：

\1. 先随机挑一个袋子

\2. 从这个袋子中随机挑一个球

如果你站在这个人旁边，你目睹了整个过程：这个人选了哪个袋子、抓出来的球是什么颜色的。然后你把每次选择的袋子和抓出来的球的颜色都记录下来（样本观察值），那个人不停地抓，你不停地记。最终你就可以通过你的记录，推测出每个袋子里每种球颜色的大致比例。并且你记录的越多，推测的就越准（中心极限定理）。

然而，抓球的人觉得这样很不爽，于是决定不告诉你他从哪个袋子里抓的球，只告诉你抓出来的球的颜色是什么。这时候，“选袋子”的过程由于你看不见，其实就相当于是一个隐变量。

隐变量在很多地方都是能够出现的。现在我们经常说的隐变量主要强调它的“latent”。所以广义上的隐变量主要就是指“不能被直接观察到，但是对系统的状态和能观察到的输出存在影响的一种东西”。

所以说，很多人在研究隐变量。以及设计出各种更优(比如如可解释、可计算距离、可定义运算等性质)的隐变量的表示。

在扯远一点儿，可能就跟manifold相关了。。

上面很多都是个人感觉和推论。不严谨，请勿尽信。欢迎相互讨论共同学习

[原文链接](https://www.zhihu.com/question/43216440/answer/156368711)
## 测试
dhkdhlkfhflhl
